# Audio-Emotion-ML-part

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

## ğŸ”Š Overview

**Audio-Emotion-ML-part** is a machine-learning pipeline built to analyze and classify emotional states from audio features. It ingests pre-extracted audio data, applies preprocessing and feature-engineering, and trains emotion-classification models to predict valence/arousal or discrete emotional labels.

This repo is part of a broader project â€” for raw audio processing and feature extraction, see the companion repo: [Audio-Emotion](https://github.com/Burden19/Audio-Emotion/).

## ğŸ¯ Motivation

Audio-based emotion recognition is a challenging yet powerful tool in affective computing, sentiment analysis, and human-computer interaction. With this project, you can:

- Experiment with different preprocessing and feature-engineering pipelines
- Compare classical machine-learning models for emotion classification
- Use the output as input for higher-level tasks (e.g. emotion-aware music recommendation, sentiment-driven content adaptation, behavioral analytics)

## ğŸ“ Repository Structure

```
/
â”œâ”€â”€ data/                   # preprocessed feature datasets (CSV / HDF5 / pickled)
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ test/
â”‚   â””â”€â”€ labels.csv
â”œâ”€â”€ notebooks/              # Jupyter notebooks for EDA and experiments
â”œâ”€â”€ src/                    # source code for data preprocessing, training & evaluation
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ features.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ models/                 # serialized trained models + metadata
â”œâ”€â”€ results/                # metrics, plots, logs
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md
```

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/Burden19/Audio-Emotion-ML-part.git
cd Audio-Emotion-ML-part
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸš€ Usage

### 1. Preprocessing & feature preparation

```bash
python src/preprocess.py --input data/raw/ --output data/features/ --config configs/preprocess_config.yaml
```

### 2. Train a classifier

```bash
python src/train.py --features data/features/train.pkl --labels data/features/labels_train.csv --model_output models/emotion_clf.pkl --config configs/train_config.yaml
```

### 3. Evaluate model performance

```bash
python src/evaluate.py --model models/emotion_clf.pkl --features data/features/test.pkl --labels data/features/labels_test.csv --report results/metrics.json --plots results/roc_curve.png
```

## ğŸ“Š Expected Outputs

- Cleaned, normalized, and transformed feature matrices
- Training logs and metrics (accuracy, F1-score)
- Saved model files
- Experiment reproducibility via configs

## ğŸ§ª Dependencies

- numpy, pandas
- scikit-learn
- librosa (optional)
- matplotlib, seaborn

## ğŸ¤ Contributing

1. Fork the repo  
2. Create a feature branch  
3. Commit changes  
4. Open a Pull Request  

## ğŸ§  Related Projects

- **Audio-Emotion**: https://github.com/Burden19/Audio-Emotion/

## ğŸ“„ License

MIT License
